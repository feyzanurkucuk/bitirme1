{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea52e15-29b4-4499-8508-42f681c3292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89219dd-a480-45cb-8494-fe754ad5c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1365f37c-9f6f-455c-941e-e4a78c6fd749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ab5363-cbb0-45b7-a8de-86a41af97d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc70c58-bdab-475d-828f-b25af4fbf590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pathlib.PosixPath"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp #pathlib.PosixPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a66779-2558-4bf0-a81e-bc07d4988a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0+cu121'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__ #it was 2.0.1+cu118 before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cf17b9-1224-487e-aacc-e6ad48ccfb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb91f6e-83a1-4950-8795-1ea0eb1c0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#exp2 ve webcam3.mp4 works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "145d14ed-cec2-4be2-b7bf-ff8d1f3d6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd yolov5 & pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a814600e-2e28-49bf-a424-3589d52cea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\feyza/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-1-20 Python-3.10.11 torch-2.1.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7055974 parameters, 0 gradients, 15.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5','custom',path= 'weights_last/best.pt', force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa1acb-24a2-4e31-9731-7cc36c836367",
   "metadata": {},
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb9a2cec-0d74-4d77-a25a-59904b653d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'video/ÅŸevval.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9802a811-f238-4ce8-8b43-8c46a43b9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'dataset3/images/nail-biting.6e625391-b6de-11ee-b94d-8091333110dc.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1df7441-6d9b-4778-a6b7-91ebcb76cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e69d6923-4ce9-41d5-9ca3-5d9e01ea5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ada4d0b-8c9f-45b3-a9bd-899a01f144b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4005a6d5-0fb4-4345-8b91-956d209fc726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOv5 <class 'models.common.Detections'> instance\n",
       "image 1/1: 480x640 1 nail-biting\n",
       "Speed: 15.5ms pre-process, 181.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "648b843f-7b42-4f4e-9285-4ae6e4250305",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'video/video6.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6162098-3c6e-4c8f-b04e-88953a07f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected label: nail-biting, Confidence: 0.83\n",
      "Detected label: nail-biting, Confidence: 0.50\n",
      "Detected label: nail-biting, Confidence: 0.68\n",
      "Detected label: nail-biting, Confidence: 0.77\n",
      "Detected label: nail-biting, Confidence: 0.73\n",
      "Detected label: nail-biting, Confidence: 0.74\n",
      "Detected label: nail-biting, Confidence: 0.72\n",
      "Detected label: nail-biting, Confidence: 0.86\n",
      "Detected label: nail-biting, Confidence: 0.84\n",
      "Detected label: nail-biting, Confidence: 0.86\n",
      "Detected label: nail-biting, Confidence: 0.88\n",
      "Detected label: nail-biting, Confidence: 0.85\n",
      "Detected label: nail-biting, Confidence: 0.83\n",
      "Detected label: nail-biting, Confidence: 0.77\n",
      "Detected label: nail-biting, Confidence: 0.58\n",
      "Detected label: nail-biting, Confidence: 0.84\n",
      "Detected label: nail-biting, Confidence: 0.82\n",
      "Detected label: nail-biting, Confidence: 0.84\n",
      "Detected label: nail-biting, Confidence: 0.84\n",
      "Detected label: nail-biting, Confidence: 0.83\n",
      "Detected label: nail-biting, Confidence: 0.83\n",
      "Detected label: nail-biting, Confidence: 0.85\n",
      "Detected label: nail-biting, Confidence: 0.86\n",
      "Detected label: nail-biting, Confidence: 0.87\n",
      "Detected label: nail-biting, Confidence: 0.83\n",
      "Detected label: nail-biting, Confidence: 0.80\n",
      "Detected label: nail-biting, Confidence: 0.81\n",
      "Detected label: nail-biting, Confidence: 0.64\n",
      "Detected label: nail-biting, Confidence: 0.84\n",
      "Detected label: nail-biting, Confidence: 0.79\n",
      "Detected label: nail-biting, Confidence: 0.81\n",
      "Detected label: nail-biting, Confidence: 0.76\n",
      "Detected label: nail-biting, Confidence: 0.76\n",
      "Detected label: nail-biting, Confidence: 0.75\n",
      "Detected label: nail-biting, Confidence: 0.70\n",
      "Detected label: nail-biting, Confidence: 0.72\n",
      "Detected label: nail-biting, Confidence: 0.76\n",
      "Detected label: nail-biting, Confidence: 0.78\n",
      "Detected label: nail-biting, Confidence: 0.77\n",
      "Detected label: nail-biting, Confidence: 0.77\n",
      "Detected label: nail-biting, Confidence: 0.71\n",
      "Detected label: nail-biting, Confidence: 0.80\n",
      "Detected label: nail-biting, Confidence: 0.79\n",
      "Detected label: nail-biting, Confidence: 0.70\n",
      "Detected label: nail-biting, Confidence: 0.74\n",
      "Detected label: nail-biting, Confidence: 0.72\n",
      "Detected label: nail-biting, Confidence: 0.75\n",
      "Detected label: nail-biting, Confidence: 0.61\n",
      "Detected label: nail-biting, Confidence: 0.64\n",
      "Detected label: nail-biting, Confidence: 0.66\n",
      "Detected label: nail-biting, Confidence: 0.79\n",
      "Detected label: nail-biting, Confidence: 0.84\n",
      "Detected label: nail-biting, Confidence: 0.82\n",
      "Detected label: nail-biting, Confidence: 0.81\n",
      "Detected label: nail-biting, Confidence: 0.72\n",
      "Detected label: nail-biting, Confidence: 0.83\n",
      "Detected label: nail-biting, Confidence: 0.63\n",
      "Detected label: nail-biting, Confidence: 0.71\n",
      "Detected label: nail-biting, Confidence: 0.77\n",
      "Detected label: nail-biting, Confidence: 0.51\n",
      "Detected label: nail-biting, Confidence: 0.80\n",
      "Detected label: nail-biting, Confidence: 0.74\n",
      "Detected label: nail-biting, Confidence: 0.82\n",
      "Detected label: nail-biting, Confidence: 0.86\n",
      "Detected label: nail-biting, Confidence: 0.80\n",
      "Detected label: nail-biting, Confidence: 0.68\n",
      "Detected label: nail-biting, Confidence: 0.90\n",
      "Detected label: nail-biting, Confidence: 0.85\n",
      "Detected label: nail-biting, Confidence: 0.83\n",
      "Detected label: nail-biting, Confidence: 0.57\n",
      "Detected label: nail-biting, Confidence: 0.87\n",
      "Detected label: nail-biting, Confidence: 0.82\n",
      "Detected label: nail-biting, Confidence: 0.80\n",
      "Detected label: nail-biting, Confidence: 0.63\n",
      "Detected label: nail-biting, Confidence: 0.76\n",
      "Detected label: nail-biting, Confidence: 0.51\n",
      "Detected label: nail-biting, Confidence: 0.51\n",
      "Detected label: nail-biting, Confidence: 0.57\n",
      "Detected label: nail-biting, Confidence: 0.64\n",
      "Detected label: nail-biting, Confidence: 0.67\n",
      "Detected label: nail-biting, Confidence: 0.66\n",
      "Detected label: nail-biting, Confidence: 0.51\n",
      "Detected label: nail-biting, Confidence: 0.61\n",
      "Detected label: nail-biting, Confidence: 0.66\n",
      "Detected label: nail-biting, Confidence: 0.77\n",
      "Detected label: nail-biting, Confidence: 0.75\n",
      "Detected label: nail-biting, Confidence: 0.81\n",
      "Detected label: nail-biting, Confidence: 0.72\n",
      "Detected label: nail-biting, Confidence: 0.72\n",
      "Detected label: nail-biting, Confidence: 0.71\n",
      "Detected label: nail-biting, Confidence: 0.51\n",
      "Detected label: nail-biting, Confidence: 0.52\n",
      "Detected label: nail-biting, Confidence: 0.75\n",
      "Detected label: nail-biting, Confidence: 0.55\n",
      "Detected label: nail-biting, Confidence: 0.83\n",
      "Detected label: nail-biting, Confidence: 0.85\n",
      "Detected label: nail-biting, Confidence: 0.80\n",
      "Detected label: nail-biting, Confidence: 0.81\n",
      "Detected label: nail-biting, Confidence: 0.79\n",
      "Detected label: nail-biting, Confidence: 0.77\n",
      "Detected label: nail-biting, Confidence: 0.77\n",
      "Detected label: nail-biting, Confidence: 0.81\n",
      "Detected label: nail-biting, Confidence: 0.87\n",
      "Detected label: nail-biting, Confidence: 0.87\n",
      "Detected label: nail-biting, Confidence: 0.87\n",
      "Detected label: nail-biting, Confidence: 0.69\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret,frame = cap.read()\n",
    "    results = model(frame)\n",
    "    labels = results.names\n",
    "    confidences = results.xyxy[0][:, 4]  # Assuming confidence information is in the 6th element\n",
    "    num_detections = len(results.xyxy[0])\n",
    "    confidence_threshold = 0.5  # Set your desired confidence threshold here\n",
    "\n",
    "    for i in range(num_detections):\n",
    "        confidence = confidences[i]\n",
    "        \n",
    "        if confidence > confidence_threshold:\n",
    "            label_id = int(results.xyxy[0][i][5])  # Assuming label information is in the 6th element\n",
    "            label = labels[label_id]\n",
    "            if(label == 'nail-biting'):\n",
    "                print(f'Detected label: {label}, Confidence: {confidence:.2f}')\n",
    "            \n",
    "    cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    if cv2.waitKey(10) & 0XFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c294cf02-8d79-4b06-8591-2f56db48f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected label: nail-biting, Confidence: 0.61\n"
     ]
    }
   ],
   "source": [
    "labels = results.names\n",
    "confidences = results.xyxy[0][:, 4]  # Assuming confidence information is in the 6th element\n",
    "num_detections = len(results.xyxy[0])\n",
    "confidence_threshold = 0.5  # Set your desired confidence threshold here\n",
    "\n",
    "for i in range(num_detections):\n",
    "        confidence = confidences[i]\n",
    "        \n",
    "        if confidence > confidence_threshold:\n",
    "            label_id = int(results.xyxy[0][i][5])  # Assuming label information is in the 6th element\n",
    "            label = labels[label_id]\n",
    "            print(f'Detected label: {label}, Confidence: {confidence:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a87a08d-37b7-4f49-b4f0-613f461768ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 52,  70,  69],\n",
       "        [ 55,  70,  69],\n",
       "        [ 57,  69,  68],\n",
       "        ...,\n",
       "        [159, 163, 166],\n",
       "        [158, 164, 168],\n",
       "        [158, 164, 168]],\n",
       "\n",
       "       [[ 57,  71,  70],\n",
       "        [ 60,  75,  73],\n",
       "        [ 58,  72,  71],\n",
       "        ...,\n",
       "        [167, 172, 171],\n",
       "        [164, 171, 170],\n",
       "        [163, 171, 170]],\n",
       "\n",
       "       [[ 55,  69,  68],\n",
       "        [ 57,  69,  68],\n",
       "        [ 60,  70,  69],\n",
       "        ...,\n",
       "        [166, 168, 168],\n",
       "        [162, 164, 164],\n",
       "        [160, 162, 162]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[108, 112, 123],\n",
       "        [113, 119, 131],\n",
       "        [108, 117, 130],\n",
       "        ...,\n",
       "        [ 29,  27,  42],\n",
       "        [ 28,  27,  41],\n",
       "        [ 25,  24,  37]],\n",
       "\n",
       "       [[106, 115, 125],\n",
       "        [105, 114, 124],\n",
       "        [105, 114, 122],\n",
       "        ...,\n",
       "        [ 33,  28,  42],\n",
       "        [ 31,  22,  41],\n",
       "        [ 26,  16,  36]],\n",
       "\n",
       "       [[104, 109, 118],\n",
       "        [107, 112, 119],\n",
       "        [104, 110, 116],\n",
       "        ...,\n",
       "        [ 29,  24,  36],\n",
       "        [ 28,  19,  38],\n",
       "        [ 22,  13,  33]]], dtype=uint8)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(results.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e923b4ac-396e-480a-90d9-1a846f47bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bd7ae38-1624-4501-8344-999664af6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.join('dataset','new_images')\n",
    "labels = ['nail-biting','normal']\n",
    "number_imgs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebf643cb-8968-44b6-a995-2f5e3ecbc215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collectiong images for nail-biting\n",
      "Collectiong images for nail-biting, image number 0\n",
      "Collectiong images for nail-biting, image number 1\n",
      "Collectiong images for nail-biting, image number 2\n",
      "Collectiong images for nail-biting, image number 3\n",
      "Collectiong images for nail-biting, image number 4\n",
      "Collectiong images for nail-biting, image number 5\n",
      "Collectiong images for nail-biting, image number 6\n",
      "Collectiong images for nail-biting, image number 7\n",
      "Collectiong images for nail-biting, image number 8\n",
      "Collectiong images for nail-biting, image number 9\n",
      "Collectiong images for nail-biting, image number 10\n",
      "Collectiong images for nail-biting, image number 11\n",
      "Collectiong images for nail-biting, image number 12\n",
      "Collectiong images for nail-biting, image number 13\n",
      "Collectiong images for nail-biting, image number 14\n",
      "Collectiong images for nail-biting, image number 15\n",
      "Collectiong images for nail-biting, image number 16\n",
      "Collectiong images for nail-biting, image number 17\n",
      "Collectiong images for nail-biting, image number 18\n",
      "Collectiong images for nail-biting, image number 19\n"
     ]
    }
   ],
   "source": [
    "images_path = os.path.join('dataset','new_images')\n",
    "labels = ['nail-biting','normal']\n",
    "number_imgs = 20\n",
    "cap = cv2.VideoCapture(0)\n",
    "label = 'nail-biting'\n",
    "print('Collectiong images for {}'.format(label))\n",
    "time.sleep(5)\n",
    "\n",
    "for img_num in range(number_imgs):\n",
    "    print('Collectiong images for {}, image number {}'.format(label,img_num))\n",
    "        #webcam feed\n",
    "    ret, frame = cap.read()\n",
    "        #naming image path\n",
    "    imgname = os.path.join(images_path, label+'.'+str(uuid.uuid1())+'.jpg')\n",
    "        #write images to file\n",
    "    cv2.imwrite(imgname,frame)\n",
    "        #render to screen\n",
    "    cv2.imshow('Image collection',frame)\n",
    "        # 2 second delay between captures\n",
    "    time.sleep(2)\n",
    "    if cv2.waitKey(10) & 0XFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b374793a-89e6-4981-ae98-6aca17d8ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nail-biting\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf2d5ac3-6f20-4a59-977f-762848308356",
   "metadata": {},
   "outputs": [],
   "source": [
    " imgname = os.path.join(images_path, label+'.'+str(uuid.uuid1())+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dde105ad-d66a-484f-bb72-2756498c2deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset\\\\new_images\\\\normal.da5b517a-b373-11ee-93a5-8091333110dc.jpg'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3a6b83-fa37-4a3b-bdd8-c65a0b9d9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5664d419-0f82-4dca-8a88-14dca87000bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a979a2-8c71-4dbd-94e7-8c1b14b59c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'dataset/images'\n",
    "label_path = 'dataset/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f740f9f-5a24-4c0c-b0a2-9f622dd48084",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(image_path) or not os.path.exists(label_path):\n",
    "    print(\"Image or label path does not exist.\")\n",
    "else:\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(image_path):\n",
    "        image_file_path = os.path.join(image_path, filename)\n",
    "        label_file_path = os.path.join(label_path, filename.replace('.jpg', '.txt'))\n",
    "\n",
    "        # Check if both image and label files exist\n",
    "        if os.path.isfile(image_file_path) and os.path.isfile(label_file_path):\n",
    "            with open(label_file_path, 'r') as label_file:\n",
    "                # Read the contents of the label file\n",
    "                label_content = label_file.read().strip()\n",
    "\n",
    "                # Append image file path to images_list and label content to labels_list\n",
    "                images_list.append(image_file_path)\n",
    "                labels_list.append(label_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32505f20-38b5-44f0-9f72-17d1c6e10b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2353"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "778c3a33-7f40-46a5-a728-2e86637f8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = labels_list[0]\n",
    "parts = line.strip().split()\n",
    "label = int(parts[0])  # assuming label is the first value in YOLO format\n",
    "xmin, ymin, width, height = map(float, parts[1:])\n",
    "xmax = xmin + width\n",
    "ymax = ymin + height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed8afffe-5baf-4580-8791-a07c57eb9db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 0.475781, 0.573958, 0.473438, 0.23125)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label, xmin, ymin, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdc39bcb-1870-4f6a-a8fb-da6aaf3fdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "output = []\n",
    "for ind in range(len(images_list)):\n",
    "    image = images_list[ind]\n",
    "    img_arr = cv2.imread(image)\n",
    "    h,w,d = img_arr.shape\n",
    "    # preprocessing \n",
    "    load_image = load_img(image,target_size=(224,224))\n",
    "    load_image_to_arr = img_to_array(load_image)\n",
    "    norm_load_image_arr = load_image_to_arr/255.0 #normalization\n",
    "\n",
    "    #normalization to labels\n",
    "    line = labels_list[ind]\n",
    "    parts = line.strip().split()\n",
    "    label = int(parts[0])  # assuming label is the first value in YOLO format\n",
    "    xmin, ymin, width, height = map(float, parts[1:])\n",
    "    xmax = xmin + width\n",
    "    ymax = ymin + height\n",
    "\n",
    "    # Calculate normalization based on image dimensions (w and h)\n",
    "    nxmin, nxmax = xmin/w, xmax/w\n",
    "    nymin, nymax = ymin/h, ymax/h\n",
    "\n",
    "    # Create normalized label\n",
    "    label_norm = (nxmin, nxmax, nymin, nymax)\n",
    "    \n",
    "    #-----------append\n",
    "    data.append(norm_load_image_arr)\n",
    "    output.append(label_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e74c1455-b9cd-46fa-9f3c-0b2fe0a72ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data,dtype = np.float32)\n",
    "y = np.array(output,dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c34ecdc0-a4a4-45a4-9163-510e23603479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2353, 224, 224, 3), (2353, 5))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f42518b4-d6fb-4d66-b58b-4f49db4610a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1882, 224, 224, 3), (471, 224, 224, 3), (1882, 5), (471, 5))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,y,train_size = 0.8,random_state=0)\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e880376-ac98-437a-885c-b0ce56bfded6",
   "metadata": {},
   "source": [
    "Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2162454c-8957-4f0e-8cfa-f8f0315b3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2, InceptionV3, InceptionResNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c04be61-2026-4830-861a-e4e6d9489ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35bca1fb-3540-407f-a77a-a7359f980967",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_resnet = InceptionResNetV2(weights = \"imagenet\",include_top=False,\n",
    "                                     input_tensor=Input(shape=(224,224,3)))\n",
    "inception_resnet.trainable=False\n",
    "\n",
    "headmodel = inception_resnet.output\n",
    "headmodel = Flatten()(headmodel)\n",
    "headmodel = Dense(500,activation = 'relu')(headmodel)\n",
    "headmodel = Dense(250,activation = 'relu')(headmodel)\n",
    "headmodel = Dense(5,activation='sigmoid')(headmodel)\n",
    "model = Model(inputs=inception_resnet.input,outputs=headmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65a8be2b-18f4-4101-97b0-cbb9a54ccbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e5dd64a-0cdd-4185-98cd-790dd3987188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "921e70c3-77f9-402b-b0e7-66637c30403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfb = TensorBoard('object_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbdaa2b-a24c-4ff1-9cee-0ccd0b91244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 40.6634"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train,y=y_train,batch_size=10,epochs=100,\n",
    "                    validation_data=(x_test,y_test),callbacks = [tfb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e2193-b8bb-4f36-af0e-95ce87270885",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/object_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "f9c6ae06-031b-4444-8d20-4bb219391685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/images\\\\frame2_0251.jpg'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b67b6d2c-15f4-491c-b1ae-e2fccf427728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing label file for image: dataset2/images\\frame4_0099.jpg\n"
     ]
    }
   ],
   "source": [
    "image_path = 'dataset2/images'\n",
    "label_path = 'dataset2/labels'\n",
    "\n",
    "# Ensure the paths exist\n",
    "if not os.path.exists(image_path) or not os.path.exists(label_path):\n",
    "    print(\"Image or label path does not exist.\")\n",
    "else:\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(image_path):\n",
    "        image_file_path = os.path.join(image_path, filename)\n",
    "        label_file_path = os.path.join(label_path, filename.replace('.jpg', '.txt'))\n",
    "\n",
    "        # Check if both image and label files exist\n",
    "        if os.path.isfile(image_file_path):\n",
    "            images_list.append(image_file_path)\n",
    "\n",
    "            # Check if the label file exists\n",
    "            if os.path.isfile(label_file_path):\n",
    "                labels_list.append(label_file_path)\n",
    "            else:\n",
    "                labels_list.append(None)  # Placeholder for missing label file\n",
    "\n",
    "    # Print the result\n",
    "    for image, label in zip(images_list, labels_list):\n",
    "        if label is None:\n",
    "            print(f\"Missing label file for image: {image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "e345caf8-e7c0-442c-9047-5aa9aca10225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "73f76552-b3ed-4133-a015-4f986a525b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_face = mp.solutions.face_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c5d05f44-ad11-47ae-b3f2-dbece5239135",
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands()\n",
    "face_detection = mp_face.FaceDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "a4192252-d87c-46b5-b947-b72eac5feb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('dataset/images/frame4_0279.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "73810f01-e142-4297-8b70-a993afa53dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Detect hands\n",
    "results_hands = hands.process(rgb_img)\n",
    "\n",
    "# Detect face\n",
    "results_face = face_detection.process(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "0083f85f-d9b4-48bb-98d6-d5ec29d3aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bbox = results_face.detections[0].location_data.relative_bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "15efd15a-e5e8-4370-ade2-82b96efa8b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xmin: 0.28120502829551697\n",
       "ymin: 0.3143937289714813\n",
       "width: 0.81508469581604\n",
       "height: 0.4612949788570404"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "bff016d3-d1a4-4dff-bd5b-376312f503fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_landmarks = results_hands.multi_hand_landmarks[0].landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f4c6b2c6-811d-455f-a15c-0f479edf5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_hands.multi_hand_landmarks and results_face.detections:\n",
    "    hand_landmarks = results_hands.multi_hand_landmarks[0].landmark\n",
    "\n",
    "    # Get face bounding box\n",
    "    face_bbox = results_face.detections[0].location_data.relative_bounding_box\n",
    "\n",
    "    # Extract mouth coordinates from the face bounding box\n",
    "    xmin = face_bbox.xmin\n",
    "    ymin = face_bbox.ymin\n",
    "    width = face_bbox.width\n",
    "    height = face_bbox.height\n",
    "\n",
    "    # Calculate mouth coordinates\n",
    "    mouth_x = int((xmin + width / 2) * img.shape[1])\n",
    "    mouth_y = int((ymin + height) * img.shape[0])\n",
    "\n",
    "    # Extract fingertip coordinates from the hand landmarks\n",
    "    fingertip_x = int(hand_landmarks[8].x * img.shape[1])\n",
    "    fingertip_y = int(hand_landmarks[8].y * img.shape[0])\n",
    "\n",
    "    # Draw circle on fingertip\n",
    "    proximity_threshold = 30  # Adjust as needed\n",
    "\n",
    "    # Check if the right hand is close to the mouth\n",
    "    is_hand_on_mouth = (\n",
    "        abs(fingertip_x - mouth_x) < proximity_threshold\n",
    "        and abs(fingertip_y - mouth_y) < proximity_threshold\n",
    "    )\n",
    "\n",
    "    if is_hand_on_mouth:\n",
    "        print(\"Hand is on the mouth!\")\n",
    "\n",
    "    # Draw circle on mouth\n",
    "    cv2.circle(img, (mouth_x, mouth_y), 10, (0, 0, 255), -1)\n",
    "\n",
    "    # Draw circle on right fingertip\n",
    "    cv2.circle(img, (fingertip_x, fingertip_y), 10, (0, 255, 0), -1)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Result', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Close mediapipe hands and face components\n",
    "hands.close()\n",
    "face_detection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a43f4-f124-498c-9fc7-8671b431faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe hands and face components\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face = mp.solutions.face_detection\n",
    "hands = mp_hands.Hands()\n",
    "face_detection = mp_face.FaceDetection()\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread('path/to/your/image.jpg')\n",
    "\n",
    "# Convert the BGR image to RGB\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Detect hands\n",
    "results_hands = hands.process(rgb_img)\n",
    "\n",
    "# Detect face\n",
    "results_face = face_detection.process(rgb_img)\n",
    "\n",
    "# Check if hands and face are detected\n",
    "if results_hands.multi_hand_landmarks and results_face.detections:\n",
    "    # Get hand landmarks\n",
    "    hand_landmarks = results_hands.multi_hand_landmarks[0].landmark\n",
    "\n",
    "    # Get face bounding box\n",
    "    face_bbox = results_face.detections[0].location_data.relative_bounding_box\n",
    "\n",
    "    # Extract mouth coordinates from the face bounding box\n",
    "    mouth_x = int((face_bbox.xmin + face_bbox.xmax) / 2 * img.shape[1])\n",
    "    mouth_y = int((face_bbox.ymin + face_bbox.ymax) / 2 * img.shape[0])\n",
    "\n",
    "    # Extract fingertip coordinates from the hand landmarks\n",
    "    fingertip_x = int(hand_landmarks[8].x * img.shape[1])\n",
    "    fingertip_y = int(hand_landmarks[8].y * img.shape[0])\n",
    "\n",
    "    # Draw circle on fingertip\n",
    "    cv2.circle(img, (fingertip_x, fingertip_y), 10, (0, 255, 0), -1)\n",
    "\n",
    "    # Draw circle on mouth\n",
    "    cv2.circle(img, (mouth_x, mouth_y), 10, (0, 0, 255), -1)\n",
    "\n",
    "    # Draw line connecting fingertip and mouth\n",
    "    cv2.line(img, (fingertip_x, fingertip_y), (mouth_x, mouth_y), (255, 0, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Result', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Close mediapipe hands and face components\n",
    "hands.close()\n",
    "face_detection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b28c88-3cdb-41be-8695-12304274c167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
